#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+TITLE: IP REPORT 02
#+AUTHOR: Li Shihao
# #+DATE: 2021-01-20
#+OPTIONS: ^:{}



A static html file is available for download [[https://www.dropbox.com/s/j3qqvshr22wg0ta/IP_report02_LiShihao.html?dl=0][here]] for the better reading experience in a
browser. Highly recommend.

* Introduction
A quick recap, this project aims to build an Instagram hashtag
recommender that suggests less hot alternatives based on the given hashtag.
The main structure is to have 3 components: a Python web scrapper that
stripes relevant info from Instagram and store the info into Database, a MySQL
database, an JavaScript interface for user interaction and data
retrieval from database. In this report, it will mainly focus on the
progress made for the Python scrapper and the database table design.

So far, my role is to build a backend scrapper logic /without/ using
~selenium~ package which demostrated in previous presnetaion.

* [[https://github.com/instagram-hashtag-analyzer/instascrapper-code][Python Scrapper üîó]]
So far the =mainlogic.py= is a fully functional command line tool to print out
the top 9 posts info of the input hashtag, kindly try it out. (potential
failure <<failure>>: I included my cookie string in the headers object for the web access,
in order to stay login -- FaceBook limits the number of search without login, may
not work for other PC)

** Basic set up

Python environment set up as below:
# # #+CAPTION: python environment
# #+NAME:   fig:env
# [[./img/env.png]]
#+begin_src shell
li@intel ~ % python --version
Python 3.8.5
li@intel ~ % which python
/Users/li/.pyenv/shims/python
li@intel ~ % pip --version
pip 20.3.3 from /Users/li/.pyenv/versions/3.8.5/lib/python3.8/site-packages/pip (python 3.8)
li@intel ~ % which pip
pip: aliased to /Users/li/.pyenv/versions/3.8.5/bin/pip
li@intel ~ %
#+end_src

=pyenv= is used to manage the default python version on my system, and =pip= as
the package manager.

Create a pyton file in the working directory (where =.git= locates too) called
=mainlogic.py=, which stores the main steps needed
for the scrapper. The entry point of the program is defined as below:

#+NAME: Entry point
#+BEGIN_SRC python
if __name__ == "__main__":
    start_time = time.time()
    start()
    print("-- execution time: %ss --" % (time.time() - start_time))
#+END_SRC

#+RESULTS: Entry point
The time function here is to calculate the program run time for internal use.
Logic will be put into =start()= function with each step to abstracted into
function as possible.

** Dependencies

The packages used as below:
#+begin_src python
import urllib.request
import urllib.error # get webpage by URL
import re
import json
import time
from datetime import datetime, date
from bs4 import BeautifulSoup # python -m pip install bs4 # parse web, obtain data
import mysql.connector
from mysql.connector import errorcode
#+end_src
[[https://www.crummy.com/software/BeautifulSoup/bs4/doc/][BeautifulSoup]](4.9.3) and [[https://dev.mysql.com/doc/connector-python/en/][mysql-connector-python]](8.0.22) are the packages that
not included in Python3 by default, the rest comes with Python. In the [[https://github.com/instagram-hashtag-analyzer/instascrapper-code][GitHub
repository]], =requirement.txt= file is provided for installing the dependencies,
which is generated by =pip freeze=, therefore extra packages are included
unnecessarily at the moment.

To install the dependencies, =cd= to the working directory and type below command:
#+begin_src shell
python -m pip install -r requirements.txt
#+end_src
Notice that =selenium= package is not required in this approach.

** Logic
The main logic of the scrapper is comprised of below parts:
    1. Get the content of the desired webpage
    2. Parse the content to pull only the relevant info
    3. Save the info to a DataBase

*** HOLD Get the content of the webpage
First we notice that to search a hashtag, the url is always "https://www.instagram.com/explore/tags/ /SomeHashtag/". We need to access the url
and rip the HTML of it. A function =get_content_url(url)= is defined for that
purpose, take url as parameter and return the whole HTML:
#+begin_src python
def get_content_url(url): # return page source HTML

    data = bytes(urllib.parse.urlencode({'name': 'user'}), encoding = "utf-8")

    cookie = '''csrftoken=LqWwmIYutdVdjfZs19lC21UJxj328lHe; ds_user_id=145399310; rur=PRN; urlgen="{\"58.182.45.73\": 55430}:1kmHZR:gTUeuOkl2OpGz-awwXp_sjFVPXs"; shbid=9296; shbts=1607340966.1711164; sessionid=145399310%3AAgVenBkUyalwhw%3A24; ig_nrcb=1; ig_did=160650D6-C2A1-47CA-B84F-1CABBEDE7C6C; mid=X0zm8AAEAAEZtMalVoXXLUz_kkgL'''

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36",
        "Cookie": cookie
    }

    request = urllib.request.Request(url=url, data=data, headers=headers, method="POST") # fine to be GET without data

    html = ""

    try:
        response = urllib.request.urlopen(request, timeout = 4)
        html = response.read().decode("utf-8")
        #print(html) # For display only
    except urllib.error.URLError as e:
        if hasattr(e, "code"):
            print(e.code)
        if hasattr(e, "reason"):
            print(e.reason)

    return html
#+end_src
It basically fakes the identity as a browser to exchange the header file during
the network request process, by making use of the =urllib= package's URL request
function, in this case HTTP POST is used.
After looking into the HTML, I found that the relevant info, of the top 9 posts,
is actually embeded in an extremely long JSON string, below attach a tiny
fraction of it:
#+begin_src html
<script type="text/javascript">window._sharedData = {"config":{"csrf_token":"LqWwmIYutdVdjfZs19lC21UJxj328lHe","viewer":{"biography":"","category_name":null,"external_url":null,"fbid":"17841401659302271","full_name":"\u524d\u65b9\u306e\u98ce","has_phone_number":true,"has_profile_pic":true,"has_tabbed_inbox":false,"id":"145399310","is_joined_recently":false,"is_private":false,"is_professional_account":false,"profile_pic_url":"https://instagram.fsin1-1.fna.fbcdn.net/v/t51.2885-19/11909348_1462841624021763_840604754_a.jpg?_nc_ht=instagram.fsin1-1.fna.fbcdn.net\u0026_nc_ohc=mwELOcJd8V4AX_dSLDO\u0026oh=5b1ccf765bae47475ac626f2dbf8ef40\u0026oe=602DD427","profile_pic_url_hd":"https://instagram.fsin1-1.fna.fbcdn.net/v/t51.2885-19/11909348_1462841624021763_840604754_a.jpg?_nc_ht=instagram.fsin1-1.fna.fbcdn.net\u0026_nc_ohc=mwELOcJd8V4AX_dSLDO\u0026oh=5b1ccf765bae47475ac626f2dbf8ef40\u0026oe=602DD427","should_show_category":false,"username":"shly0718","badge_count":"{\"seq_id\": 836, \"badge_count\": 0, \"badge_count_at_ms\": 1610866480067}"},"viewerId":"145399310"},"country_code":"SG","language_code":"en","locale":"en_US","entry_data":{"TagPage":[{"graphql":{"hashtag":{"id":"17843958460043944","name":"apple","allow_following":true,"is_following":false,"is_top_media_only":false,"profile_pic_url":"https://instagram.fsin1-1.fna.fbcdn.net/v/t51.2885-15/e35/s150x150/139246320_235310888105678_4224857578424497632_n.jpg?_nc_ht=instagram.fsin1-1.fna.fbcdn.net\u0026_nc_cat=102\u0026_nc_ohc=GxelIGXlp2wAX8GR2Qs\u0026tp=1\u0026oh=eaa144b23758e5efd656b410ac8c92b7\u0026oe=602CA56A","edge_hashtag_to_media":{"count":35435496,"page_info":
#+end_src
Which means there is parsor mechanism built in the browser, or the JavaScrit on the
page to interprate it, so that the page can't be loaded with information like
number of likes and comments.

So instead of the whole page of HTML, let's trim the JSON string instead, below
=get_tagpage_json= wraps the =get_content_url= function, then use the
=BeautifulSoup= package to parse the HTML to locate to the desired keyword
argument, the content under 4^{th} =<script type ="text/javascript">= in this case.
#+begin_src python
def get_tagpage_json(tag_url):
    html = get_content_url(tag_url)

    soup = BeautifulSoup(html, "html.parser") # parser object

    list_scrpit = soup.find_all("script", type="text/javascript")

    jsonstr = list_scrpit[3].string[20:-1]

    return jsonstr
#+end_src
Putting this function in the main logic body instead of =get_content_url= makes
the step 1 cleaner with 1 line code.

*** Parse the JSON string
So far, we have obtained the JSON string. In this step, we need to search and
take only the portion we want.

Here we utilize =re= package which enables regular expression for searching in
string. For example, we only need the part where involves the info of top 9
posts, which is behind keyword "edge_hashtag_to_top_posts", I chose to use
=re.search= instead of =re.findall=, because it only matches the first
occurrence of a pattern, but returns a weird match object. =.group(0)= used here
to access the string (hard coded to remove some unwanted characters):
#+begin_src python
toppost_str = re.search('"edge_hashtag_to_top_posts":.*},"edge_hashtag_to_content_advisory', jsonstr).group(0)[28 : -34]
#+end_src
By the way, the total number of likes of a hashtag is not part of top9 posts
info, the number is behind keyword "edge_hashtag_to_media":
#+begin_src python
total_like_str = re.search('"edge_hashtag_to_media":{"count":(\d+)', jsonstr).group(0) #
tottal_like_count : int = int(total_like_str.split(':')[2]) # Milestone
#+end_src

Also to make the JSON string readable, I save the top 9 posts info into a JSON
file, =json.load= and =json.dump= are from =json= package:
#+begin_src python
toppost_dicts = json.loads(toppost_str)
with open('raw_top.json', 'w', encoding='utf-8') as jsonfile:
        json.dump(toppost_dicts, jsonfile, indent=4, ensure_ascii=False)
#+end_src
The fraction of the file looks like this, I save the file under the same working
directory, but it's for viewing only, not really essential for the program:
#+begin_src json
{
    "edges": [
        {
            "node": {
                "__typename": "GraphImage",
                "id": "2486504591227683678",
                "edge_media_to_caption": {
                    "edges": [
                        {
                            "node": {
                                "text": "Setup for today! \nMaking my favourite Devices Float a lil! üòú\nBeen using the Magic keyboard for a long time and still loving it! \n\nAlways keeping a Notepad on my Desk because I like to write and schedule my work/ photo shoots in prior. üìù"
                            }
                        }
                    ]
                },
                "shortcode": "CKB1sBsgQNe",
                "edge_media_to_comment": {
                    "count": 44
                },
                "taken_at_timestamp": 1610634467,
                "dimensions": {
                    "height": 1080,
                    "width": 1080
                },
                "display_url": "https://instagram.fsin1-1.fna.fbcdn.net/v/t51.2885-15/e35/s1080x1080/138569923_3783952161643701_8598433727873440451_n.jpg?_nc_ht=instagram.fsin1-1.fna.fbcdn.net&_nc_cat=109&_nc_ohc=cGCtrdq4XQ0AX-fdJ87&tp=1&oh=606618aff214684ac1dbda5e48ca4ab1&oe=6029F015",
                "edge_liked_by": {
                    "count": 1062
                },
                "edge_media_preview_like": {
                    "count": 1062
                },
                "owner": {
                    "id": "362783038"
                },
                "thumbnail_src": "https://instagram.fsin1-1.fna.fbcdn.net/v/t51.2885-15/sh0.08/e35/s640x640/138569923_3783952161643701_8598433727873440451_n.jpg?_nc_ht=instagram.fsin1-1.fna.fbcdn.net&_nc_cat=109&_nc_ohc=cGCtrdq4XQ0AX-fdJ87&tp=1&oh=d1a2483a64917efca90d4a6eb8b44690&oe=6029437E",
#+end_src
I haven't implemented to store the desired info into variables then DataBase, I
print to the console as of now, those keywords are pretty self-explanatory:
#+begin_src python
 print("PostID\t\tLikes\tComments\tDate")
 for dict in toppost_dicts["edges"]:
     print(dict["node"]["shortcode"] + '\t'
         + str(dict["node"]["edge_liked_by"]["count"]) + '\t'
         + str(dict["node"]["edge_media_to_comment"]["count"]) + '\t\t'
         + datetime.utcfromtimestamp(dict["node"]["taken_at_timestamp"]).strftime('%Y-%m-%d')
     ) # for display only
#+end_src

*** STRT Database manipulation
The creation of database should be done in backend instead of frontend. So far
we have create one sample database called =explore= and a table called
=hashtag=. The table attributes so far are just samples, we shall discuss more
in the next section.

The database operation relies on [[https://dev.mysql.com/doc/connector-python/en/][mysql-connector-python]] package and its manual.
**** *Create database*
First I wrote a function to create database and catch the exception if the
database exists:
#+begin_src python
def create_database(DB_NAME):
    try:
        newdb = mysql.connector.connect(
        host     = "localhost",
        user     = "root",
        password = "TIC3901"
        )

        newcursor = newdb.cursor()

        newcursor.execute(
            "CREATE DATABASE {};".format(DB_NAME))
        print("New Database Created!")
    except mysql.connector.Error as e:
        if e.errno == errorcode.ER_ACCESS_DENIED_ERROR:
            print("Something is wrong with your user name or password")
        else:
            print("Preparing Database...")
            pass
#+end_src
Note that the MySQL's user is `root`, password is `TIC3901`.


**** *Use/connect database*
Create connection and cursor in the similiar fashion.
#+begin_src python
 conn = mysql.connector.connect(
     host      = "localhost",
     user      = "root",
     password  = "TIC3901",
     database  =  DB_NAME,
     charset   = "utf8",
     collation = "utf8_general_ci"
 )

 cursor = conn.cursor()
#+end_src

#+RESULTS:

**** *Create table (01)*

The table attributes design is still up to change.
#+begin_src python
try:
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS `hashtag` (
        `name` varchar(50) COLLATE utf8_unicode_ci NOT NULL,
        `numPost` int(12) NOT NULL,
        `top9PostId` varchar(50) COLLATE utf8_unicode_ci,
        `createDate` datetime NOT NULL DEFAULT current_timestamp()
        )
    ''')

except mysql.connector.Error as e:
    print("Table error: ")
    print(e.msg)
    pass
#+end_src

**** TODO *Insert sample data*
So far, sample data is tried here to fill the table for testing purpose, seems
data is able to fill in but no exception handling yet for duplicate entry error.
#+begin_src python
add_tag = ("INSERT INTO hashtag "
                    "(name, numPost, createDate) "
                    "VALUES (%(name)s, %(numPost)s, %(createDate)s)")

# data set for test only, TODO: data feeder implementation
data_tag = {
    'name':       tag_name,
    'numPost':    tottal_like_count,
    'createDate': date(2021, 1, 1)
}

cursor.execute(add_tag, data_tag)

cursor.execute('''
    ALTER TABLE `hashtag`
    ADD PRIMARY KEY (`name`);
''')

conn.commit()
cursor.close()
conn.close()
#+end_src

=SELECT * FROM hashtag= in terminal =mysql= shows below table:
+--------+------------+------------+---------------------+
| name   | numLike    | top9PostId | createDate          |
+--------+------------+------------+---------------------+
| apple  |   35465424 | NULL       | 2021-01-01 00:00:00 |
| love   | 2000022138 | NULL       | 2021-01-01 00:00:00 |
| winter |  147962789 | NULL       | 2021-01-01 00:00:00 |
+--------+------------+------------+---------------------+


* WAIT Database design
The important experience gained from this project is that we should probaly have
done the database schema design at the very beginning. Once the tables and
their attributes are fixed, backend and frontend can have better seperation
without worrying affect each other.

The table design is still up to discussion, but I'd propose as of now:

#+CAPTION: hashtag
| name   |    numLike |
|--------+------------|
| apple  |   35465424 |
| love   | 2000022138 |
| winter |  147962789 |
=name= should be primary key as each hashtag and the search page URL are unique.

#+CAPTION: tag-toppost
| tag_name | postId        |
|----------+---------------|
| coding   | =CKQLGFVAo6e= |
| coding   | =CKTPbS6A0mg= |
| coding   | =CKPFztCDsUO= |
| coding   | =CKRtAtvgWFO= |
| coding   | =CKRZ13ygtuw= |
| coding   | =CKR7KFCFD7d= |
| coding   | =CKSs0yfAciw= |
| coding   | =CKRLDmaAxmR= |
| coding   | =CKSwFaNAsWr= |

#+CAPTION:toppost
| postId        | numLike | numComment |       Date |
|---------------+---------+------------+------------|
| =CKQLGFVAo6e= |    8892 |         52 | 2021-01-20 |
| =CKTPbS6A0mg= |    2799 |        223 | 2021-01-21 |
| =CKPFztCDsUO= |    1426 |         14 | 2021-01-19 |

Above are the info I can crawl at the moment, I'm confident that I'm able to
parse the JSON string, to get the comment section of each post and extract all
the tags tagged along with the post.

#+CAPTION: post_contain_tag
| postId        | tagname       |
|---------------+---------------|
| =CKPFztCDsUO= | NULL          |
| =CJ1d_NZgli1= | peoplewhocode |
| =CJ1d_NZgli1= | 100daysofcode |
| =CJ1d_NZgli1= | buildupdevs   |

Not 100% sure if it is the right approach to find the relation to the other
possible hashtags.

* Future planning and challenges

** KILL Login and maintain the session?
As mentioned [[failure][above]], because of the Facebook request limitation, we can't keep
performing searches without login to Instagram. I included my own cookie string
to the header file, but it's unlikely to work on other machines.
One way is to have a login page in frontend to input Instagram username and
password, then my python file can retrive the info to login to the site. From my
preliminary research, to remain the state of login, =requests= package is needed
to create a session.
Another idea from Hui Yuan is that we could input a list of common hastags, my
python program would iterate through and feed the data to Database, so users making
searches in frontend have a high chance to get the existing result directly from
database, instead of running python scrapper on demand.

** WAIT Database design
To discuss on above schema design idea.

** TODO Nautural language dictionary?
From the beginning of the project, I have been struggling to come up with
anything to tell 2 hashtags having similiar meaning. Because we want to
recommend to users a set of relevant hashtags if the one they chose was too popular.
Or does the close meaning of the hashtags really matter much? For example, I saw
post with [[https://www.instagram.com/explore/tags/coding/][#coding]], also tagged along with [[https://www.instagram.com/explore/tags/linux][#linux]], [[https://www.instagram.com/explore/tags/java][#java]], [[https://www.instagram.com/explore/tags/python][#python]], [[https://www.instagram.com/explore/tags/javascript][#javascript]]
and more. You can't say those are of close meaning or high relevancy. We are
also unlikely to understand the content of what users are going to post other than the
user input hashtag, so defining the relevancy is already problematic. So shall
we just ultilise the last table or other measures as the consideration to
determine the relation between tags?

* Misc reflections
** Easy way to start may not be the best way
There are 6 skill courses come with Industrial Prctice, one of them is to teach
us using Jupyter Notebook (content is nice and useful). The first preparation was
to install a software called Anaconda, which hosts the whole environment of what
we need for the course. So it is very easy and neat as we don't have to get
hands dirty to install python and so on.

At the time, I already spent quite some research time to install python
[[https://opensource.com/article/19/5/python-3-default-mac][properly]] via =pyenv=, and made the then latest python 3.8.5 and pip 3 as system default.
Although I must admit that I'm lack of knowledge on virtual environments. I
didn't know that Anaconda comes with python and it takes over as my system's
default and the =conda= is another package manager.

So after the course, at certain point of time, my python program can't run
properly due to can not find the packages that I import. It took me quite a few
weeks to realize what was going on.

Even though some tools are easy to use for beginners, if we don't what happen
underneath, it's difficult to diagnose the reasons of abnormal behaviors of the
software system.

** Docs writting
In report 01, I didn't know the Indusial Prctice website only takes pdf file for
uploads. So I wrote the report in markdown and planned to upload the =.md= file,
I'm sure there are softwares to render markdown beauifully on prof's computer.
This time, I was planning to write LaTeX as the final format has to be pdf.
However, HTML may still offer more flexible layout with CSS. So this is my first
practice to use Emacs Org Mode, despite of the steep learning curve, it
turns out to be fantastic.
